6.5840 2025 講義1: 概要

6.5840: 分散システム工学

「分散システム」の意味：
  サービスを提供するために協調する一群のコンピュータ

例
  私たちは皆分散システムを使用している
    人気アプリのバックエンド、例：メッセージング用
    大規模ウェブサイト
    電話システム
  ここでの焦点は分散インフラストラクチャ：
    ストレージ
    トランザクションシステム
    「ビッグデータ」処理フレームワーク
    認証サービス

このような方法でシステムを構築するのは容易ではない：
  並行性
  複雑な相互作用
  性能のボトルネック
  部分的故障

では、なぜ人々は分散システムを構築するのか？
  並列処理による容量の増加
  レプリケーションによる障害耐性
  物理デバイス（センサーなど）の分散への対応
  分離によるセキュリティの向上

なぜこのトピックを学ぶのか？
  興味深い - 困難な問題、強力な解決策
  広く使用されている - 大規模ウェブサイトの台頭による
  活発な研究分野 - 重要な未解決問題
  構築が困難 - ラボで実際に行うことになる

コース構造

http://pdos.csail.mit.edu/6.5840

コーススタッフ：
  Frans KaashoekとRobert Morris、講師
  Kenneth Choi、TA
  Yun-Sheng Chang、TA
  Ivy Wu、TA
  Aryan Kumar、TA

コース構成要素：
  講義
  論文
  試験2回
  ラボ
  最終プロジェクト（任意）

講義：
  大きなアイデア、論文の議論、ラボのガイダンス

論文：
  講義につき1つ
  研究論文、古典的なものと新しいものの両方
  問題、アイデア、実装詳細、評価
  授業前に論文を読んでください！
  ウェブサイトには各論文について答える短い質問があります
  そして論文について持つ質問を送ってもらいます
  講義開始前に回答と質問を提出してください

試験：
  授業内中間試験
  最終週の期末試験
  主に論文とラボについて
  試験には必ず出席してください！

ラボ：
  目標：重要な技術を使用し実装する
  目標：分散プログラミングの経験
  最初のラボは金曜日から1週間後が締切
  その後しばらくは週に1つ

ラボ1：分散ビッグデータフレームワーク（MapReduceなど）
ラボ2：クライアント/サーバー対信頼性のないネットワーク
ラボ3：レプリケーションを使用した障害耐性（Raft）
ラボ4：障害耐性のあるデータベース
ラボ5：シャーディングによるスケーラブルなデータベース性能

テストセットを使用してラボを評価します
  すべてのテストを提供します；秘密のものはありません

最終的に2〜3人のグループでの任意最終プロジェクト。
  最終プロジェクトはラボ5の代替となります。
  プロジェクトを考えて私たちと確認してください。
  コード、短いレポート、最終日のデモ。

警告：ラボのデバッグには時間がかかる場合があります
  早めに始めてください
  Piazzaで質問してください
  TAのオフィスアワーを利用してください

主要トピック

これはアプリケーション向けインフラストラクチャに関するコースです。
  * ストレージ。
  * 通信。
  * 計算。

大きな目標：分散の複雑さをアプリケーションから隠す。

トピック：障害耐性
  数千のサーバー、大規模ネットワーク -> 常に何かが故障
    これらの故障をアプリケーションから隠したい。
    「高可用性」：故障にもかかわらずサービスが継続
  大きなアイデア：レプリケートされたサーバー。
    1つのサーバーがクラッシュしても、他のサーバーを使用して続行できる。

トピック：一貫性
  汎用インフラストラクチャには明確に定義された動作が必要。
    例：「read(x)は最新のwrite(x)からの値を返す。」
  良い動作を実現するのは困難！
    例：「レプリカ」サーバーを同一に保つのは困難。

トピック：性能
  目標：スケーラブルなスループット
    N倍のサーバー -> 並列CPU、RAM、ディスク、ネットワークによるN倍の総スループット。
  Nが増加するとスケーリングが困難に：
    負荷の不均衡。
    N個のうち最も遅いものの遅延。

トピック：トレードオフ
  障害耐性、一貫性、性能は敵同士。
  障害耐性と一貫性には通信が必要
    例：データをバックアップサーバーに送信
    例：キャッシュされたデータが最新かチェック
    通信はしばしば遅くスケールしない
  多くの設計は速度を得るために一貫性を犠牲にする。
    例：read(x)が最新のwrite(x)を返さない場合がある！
    アプリケーション開発者（またはユーザー）には苦痛。
  一貫性/性能スペクトラムの多くの設計点を見ることになる。

トピック：実装
  RPC、スレッド、並行性制御、設定。
  ラボ...

この内容は実世界で多く現れる。
  すべての大規模ウェブサイトとクラウドプロバイダーは分散システムの専門家。
  多くのオープンソースプロジェクトがこれらのアイデアを基盤としている。
  学術界と産業界の両方で注目のトピック。

ケーススタディ：MapReduce

MapReduce（MR）について話そう
  6.5840の主要トピックの良い説明
  非常に影響力がある
  ラボ1の焦点

MapReduce概要
  コンテキスト：マルチテラバイトデータセットでの数時間の計算
    例：検索インデックスの構築、ソート、ウェブ構造の分析
    数千のコンピュータでのみ実用的
  大きな目標：非専門プログラマーにとって簡単
    プログラマーはMapとReduce関数を定義するだけ
    しばしばかなりシンプルなシーケンシャルコード
  MRは分散のすべての側面を管理し、隠す！
  
MapReduceジョブの抽象的な視点 -- 単語数
  Input1 -> Map -> a,1 b,1
  Input2 -> Map ->     b,1
  Input3 -> Map -> a,1     c,1
                    |   |   |
                    |   |   -> Reduce -> c,1
                    |   -----> Reduce -> b,2
                    ---------> Reduce -> a,2
  1) 入力は（すでに）M個の断片に分割されている
  2) MRは各入力断片に対してMap()を呼び出し、k,vペアのリストを生成
     「中間」データ
     各Map()呼び出しは「タスク」
  3) Mapが完了すると、
     MRは各kに対するすべての中間vを収集し、
     各キー + 値をReduce呼び出しに渡す
  4) 最終出力はReduce()からの<k,v>ペアのセット

単語数コード
  Map(d)
    dを単語に分割
    各単語wに対して
      emit(w, "1")
  Reduce(k, v[])
    emit(len(v[]))

MapReduceは良くスケールする：
  N個の「ワーカー」コンピュータで（おそらく）N倍のスループットを得られる。
    Map()は相互作用しないので並列実行可能。
    Reduce()も同様。
  したがって、より多くのコンピュータ -> より多くのスループット -- 非常に良い！

MapReduceは多くの複雑さを隠す：
  map+reduceコードをサーバーに送信
  どのタスクが完了したかを追跡
  MapからReduceへの中間データの「シャッフリング」
  サーバー間での負荷バランス
  クラッシュしたサーバーからの回復

これらの利益を得るため、MapReduceはアプリケーションを制限する：
  相互作用や状態なし（中間出力以外）。
  データフローのためのMap/Reduceパターンのみ。
  リアルタイムやストリーミング処理なし。

詳細（論文の図1）

入力と出力はGFSクラスターファイルシステムに保存される
  MRには巨大な並列入力と出力スループットが必要。
  GFSは多くのサーバー、多くのディスクに64MBチャンクでファイルを分割
    Mapは並列読み込み
    Reduceは並列書き込み
  GFSは障害耐性のためすべてのデータを2または3のサーバーにレプリケート
  GFSはMapReduceにとって大きな勝利

「コーディネーター」がジョブのすべてのステップを管理する。
  1. コーディネーターはすべてのMapが完了するまでワーカーにMapタスクを与える
     Mapは出力（中間データ）をローカルディスクに書き込む
     Mapは出力をhash(key) mod Rで分割し、Reduceタスクごとに1つのファイルに
  2. すべてのMapが終了後、コーディネーターはReduceタスクを配布
     各Reduceタスクは中間出力の1つのハッシュバケットに対応
     各Reduceタスクはすべての Mapワーカーからそのバケットを取得
     キーでソートし、各キーに対してReduce()を呼び出し
     各ReduceタスクはGFSに別個の出力ファイルを書き込み

性能を制限する可能性が高いのは何か？
  その制限が最適化すべきものなので気にする。
  CPU？メモリ？ディスク？ネットワーク？
  2004年の著者はネットワーク速度によって制限されていた。
    MRはネットワーク上で何を送信するか？
      MapはGFSから入力を読み込む。
      ReduceはMapの中間出力を取得 -- シャッフル。
        しばしば入力と同じくらい大きい、例：ソート用。
      ReduceはGFSに出力ファイルを書き込む。
    [図：サーバー、ネットワークスイッチのツリー]
    MRのall-to-allシャッフルでは、トラフィックの半分がルートスイッチを通る。
    論文のルートスイッチ：毎秒100〜200ギガビット、総計
      1800マシン、つまり毎秒55メガビット/マシン。
      55は小さい：ディスクやRAM速度より低い。

MRはどのようにネットワーク使用を最小化するか？
  コーディネーターは各Mapタスクをその入力を保存するGFSサーバー上で実行しようとする。
    すべてのコンピュータがGFSとMRワーカーの両方を実行
    したがってMap入力は通常ローカルディスクのGFSデータから読み込まれ、ネットワーク経由ではない。
  中間データはネットワークを1度だけ通過する。
    Mapワーカーはローカルディスクに書き込む。
    Reduceワーカーはネットワーク経由でMapワーカーディスクから読み込む。
    （GFSに保存すると少なくとも2回のネットワーク移動が必要。）
  中間データは多くのキーを保持するファイルにハッシュ分割される。
    Reduceタスクの単位はハッシュバケット（単一キーだけではなく）。
    大きなネットワーク転送の方が効率的。

MRはどのように良い負荷バランスを得るか？
  なぜ負荷バランスを気にするか？
    1つのサーバーが他より多くの作業を持つ、または遅い場合、
    他のサーバーは最後に待機してアイドル（無駄）になる。
  しかしタスクはサイズが異なり、コンピュータは速度が異なる。
  解決策：ワーカーマシンより多くのタスク。
    コーディネーターは前のタスクを完了したワーカーに新しいタスクを渡す。
    したがって高速サーバーは低速サーバーより多くのタスクを行う。
    そして低速サーバーには少ない作業を与え、総時間への影響を減らす。

障害耐性はどうか？
  ワーカーコンピュータがクラッシュしたら？
  アプリケーションプログラマーから故障を隠したい！
  MRは最初から全ジョブを再実行する必要があるか？
    なぜではないか？
  MRは失敗したMap()とReduce()のみを再実行する。

MRがMapタスクを2回実行し、1つのReduceは最初の実行の出力を見るが、
    別のReduceは2回目の実行の出力を見る場合を仮定？
  2つのMap実行は同じ中間出力を生成しなければならない！
  MapとReduceは純粋な決定論的関数であるべき：
    引数/入力のみを見ることが許可される。
    状態なし、ファイルI/Oなし、相互作用なし、外部通信なし、
      乱数なし。
  プログラマーはこの決定論を保証する責任がある。

ワーカークラッシュ回復の詳細：
  * Mapタスク実行中にワーカーがクラッシュ：
    コーディネーターはワーカーが応答しないことに気づく
    コーディネーターはそのワーカー上で実行されたMapタスクを知っている
      それらのタスクの中間出力は今や失われ、再作成が必要
      コーディネーターは他のワーカーにそれらのタスクを実行するよう指示
    すべてのReduceが中間データを取得済みなら再実行を省略可能
  * Reduceタスク実行中にワーカーがクラッシュ：
    完了したタスクは問題なし -- レプリカ付きでGFSに保存。
    コーディネーターはワーカーの未完了タスクを他のワーカーで再開始。

その他の故障/問題：
  * コーディネーターが2つのワーカーに同じMap()タスクを与えたら？
    おそらくコーディネーターが間違って1つのワーカーが死んだと思った。
    Reduceワーカーにはそのうち1つだけについて伝える。
  * コーディネーターが2つのワーカーに同じReduce()タスクを与えたら？
    両方ともGFS上の同じ出力ファイルに書き込もうとする！
    アトミックGFS renameが混合を防ぐ；1つの完全ファイルが見える。
  * 1つのワーカーが非常に遅い場合 -- 「ストラグラー」？
    おそらく不安定なハードウェアが原因。
    コーディネーターは最後の少数タスクの2番目のコピーを開始。
  * ワーカーが壊れたh/wやs/wのため間違った出力を計算したら？
    残念！MRは「fail-stop」CPUとソフトウェアを仮定。
  * コーディネーターがクラッシュしたら？

性能？
  図2
  X軸は時間
  Y軸は「grep」スタイルジョブがその入力を読む総速度
  1テラバイト（1000 GB）の入力
  1764ワーカー
  30,000 MB/s（30 GB/s）は巨大！
  なぜ30,000 MB/s？
    ワーカーマシンあたり17 MB/s -- 毎秒140メガビット
    私たちのネットワーク帯域幅の推測（55 mbit/s）より多い
    入力はおそらくローカルGFSディスクから直接読み込み
    したがってディスクはおそらく毎秒約17 MBで読み込み可能
  主な活動期間が約30秒なのはなぜ？
  スループットが最大に達するまで50秒かかるのはなぜ？

現在の状況？
  非常に影響力がある（Hadoop、Spark、その他）。
  おそらくGoogleでは使用されていない。
    Flume / FlumeJavaで置き換えられた（Chambersらの論文参照）。
    GFSはColossus（良い説明なし）とBigTableで置き換えられた。

結論
  MapReduceは大規模クラスター計算を人気にした。
  - 最も効率的でも柔軟でもない。
  + よくスケールする。
  + プログラムしやすい -- MRは故障とデータ移動を隠す。
  これらは実用的に良いトレードオフだった。
  コース後半でより高度な後継を見ることになる。
  ラボ1を楽しんで！