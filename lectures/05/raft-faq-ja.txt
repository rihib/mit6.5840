# Raft FAQ

Q: 人々は何のためにRaftを使いますか？

A: Raft（およびPaxos）の最も頻繁な用途は、大規模なデプロイメントにおいて
責任がどのようにサーバーに現在割り当てられているかを追跡する
フォルトトレラントな「構成サービス」を構築することです。この仕事は、
レプリケーションを使用するデプロイメントで特に重要です; Raftベースの
構成サービスは、スプリットブレインを回避する方法でプライマリを選択するため
によく使用されます。VMware FTのtest-and-setサーバーは、構成サービスの
簡単な例です。Chubby、ZooKeeper、etcdは、RaftまたはPaxosに基づく
より強力なフォルトトレラント構成サービスであり、広く使用されています。

Spanner、CockroachDB、Lab 3などのいくつかのデータベースは、データを
レプリケートするためにRaftまたはPaxosを使用します。（対照的に、GFS、
VMware FT、Chain Replicationは、データにより単純なプライマリバックアップ
を使用します。）一部のデータベースは、RaftまたはPaxosを2つの異なる方法で
使用します：各シャードに対して責任をサーバーに割り当てる構成サービス
（現在誰がプライマリで誰がバックアップか）と、各シャード内のデータを
処理するために別々に使用します。

----------

Q: Raftは単純さのために何かを犠牲にしていますか？

A: Raftは明確さの代わりに一部のパフォーマンスを諦めています; 例えば：

* すべての操作は永続性のためにディスクに書き込まれる必要があります;
  パフォーマンスには、多くの操作を各ディスク書き込みにバッチ処理すること
  が必要でしょう。

* リーダーから各フォロワーへの単一のAppendEntriesのみが有効に転送できます:
  フォロワーは順序外のAppendEntriesを拒否し、送信者のnextIndex[]機構は
  一度に一つずつを必要とします。多くのAppendEntriesをパイプライン処理
  するための規定があった方が良いでしょう。

* スナップショット設計は、状態全体をディスクに書き込むため、比較的小さな
  状態に対してのみ実用的です。状態が大きい場合（例：大きなデータベースの
  場合）、最近変更された状態の一部のみを書き込む方法が必要でしょう。

* 同様に、完全なスナップショットを送信することで回復中のレプリカを最新に
  することは遅く、レプリカがすでにわずかに古いだけのスナップショットを
  持っている場合は不必要に遅くなります。

* 操作は一度に一つずつ（ログ順序で）実行される必要があるため、サーバーは
  マルチコアを十分に活用できない可能性があります。

これらはRaftを変更することで修正できますが、結果はチュートリアルとして
の価値が低くなる可能性があります。

----------

Q: Raftは実世界のソフトウェアで使用されていますか、それとも企業は一般的に
独自のPaxosバリエーションを開発しますか（または異なるコンセンサス
プロトコルを使用しますか）？

A: Raftにはいくつかの実世界のユーザーがいます：Docker
(<https://docs.docker.com/engine/swarm/raft/>)、etcd
(<https://etcd.io>)、MongoDB。Raftを使用していると言われている他の
システムには、CockroachDB、RethinkDB、TiKVがあります。
<http://raft.github.io/>から始めてもっと見つけることができるかもしれません。

一方、多くの実世界のステートマシンレプリケーションシステム（Googleの
Chubby、ZooKeeperのZAB）は、より古いMulti-PaxosとViewstamped
Replicationプロトコルから派生しています。

----------

Q: Paxosとは何ですか？どのような意味でRaftはより単純ですか？

A: Paxosは、サーバーのセットが単一の値について合意することを可能にする
プロトコルです。Paxosを理解するには多少の思考が必要ですが、Raftよりも
はるかに単純です。Paxosについての読みやすい論文はこちらです：

<http://css.csail.mit.edu/6.824/2014/papers/paxos-simple.pdf>

しかし、PaxosはRaftよりも小さな問題を解決します。実世界のレプリケート
されたサービスを構築するために、レプリカは値の無期限のシーケンス
（クライアントコマンド）について合意する必要があり、サーバーがクラッシュ
して再起動したり、メッセージを見逃したりしたときに効率的に回復する方法
が必要です。人々はPaxosを出発点としてそのようなシステムを構築しました;
GoogleのChubbyとPaxos Made Live論文、ZooKeeper/ZABを調べてください。
Viewstamped Replicationと呼ばれるプロトコルもあります; それは良い設計で、
Raftに似ていますが、それについての論文は理解するのが困難です。

これらの実世界のプロトコルは複雑で、（Raftが登場する前は）どのように
機能するかを説明する良い入門論文がありませんでした。対照的に、Raft論文
は比較的読みやすく、かなり詳細です。それは大きな貢献です。

Raftプロトコルが本質的に他のものよりも理解しやすいかどうかは明確では
ありません。この問題は、他の実世界のプロトコルの良い説明の不足によって
曇らされています。さらに、Raftはいくつかの方法で明確さのためにパフォー
マンスを犠牲にしています; それはチュートリアルには良いですが、実世界の
プロトコルでは常に望ましいとは限りません。

----------

Q: 著者がRaftを作成する前に、Paxosはどのくらい存在していましたか？

A: Paxosは1980年代後半に発明されました。Raftは2012年頃に開発されました。

RaftはViewstamped Replicationと呼ばれるプロトコルに非常に似ており、
最初は1988年に公開されました。1990年代初頭にViewstamped Replication
の上に構築されたレプリケートされたフォルトトレラントファイルサーバー
がありましたが、本番環境では使用されませんでした。

多くの実世界のシステムがPaxosから派生しています：Chubby、Spanner、
Megastore、Zookeeper/ZAB。2000年代初頭から、大きなWebサイトと
クラウドプロバイダーはフォルトトレラントサービスを必要とし、Paxosは
その時に掘り起こされ、本番環境に投入されました。

----------

Q: 実世界のアプリケーションでRaftのパフォーマンスはPaxosと比較してどうですか？

A: 最も速いPaxos派生プロトコルは、おそらく論文で説明されているRaftよりも
高速です; ZAB/ZooKeeperとPaxos Made Liveを見てください。一方、
etcd3（Raftを使用）は、zookeeperと多くのPaxosベースの実装よりも
良いパフォーマンスを達成したと主張しています
(<https://www.youtube.com/watch?v=hQigKX0MxPw>)。

Raftのリーダーがそれほど良くない状況があります。レプリカとクライアント
を含むデータセンターが互いに遠く離れている場合、人々は時々、元のPaxos
から派生した合意プロトコルを使用します。理由は、Paxosにはリーダーがない
ことです; どのレプリカも合意を開始できます; そのため、クライアントは
遠いデータセンターのリーダーと話す必要がなく、ローカルデータセンターの
レプリカと話すことができます。ePaxosは一例です。

----------

Q: なぜPaxosの代わりにRaftを学習/実装しているのですか？

A: 6.824でRaftを使用している理由は、Raftを使用して完全なレプリケート
されたサービスを構築する方法を明確に説明している論文があるからです。
Paxosに基づいて完全なレプリケートされたサーバーシステムを構築する方法
を説明する満足のいく論文を知りません。

----------

Q: クラスターの少数のみがアクティブなときに生存し、動作し続けることが
できるRaftのようなシステムはありますか？

A: Raftの特性では不可能です。しかし、異なる仮定や異なるクライアント
から見えるセマンティクスで行うことができます。基本的な問題はスプリット
ブレインです -- レプリカの複数のサブセットが互いを認識せずに状態を
変更することによって引き起こされる、状態の複数の分岐コピーの可能性。
私が知っている2つの解決アプローチがあります。

何らかの方法でクライアントとサーバーがどのサーバーが生きていてどれが
死んでいるかを正確に学ぶことができれば（ネットワーク障害のために
生きているが到達不可能であるのと対照的に）、1つが生きている限り機能
できるシステムを構築でき、生きていることが知られている最も小さな番号の
サーバーを（例えば）選択できます。ただし、通常、1台のコンピュータが
別のコンピュータが死んでいるかどうかを決定することは、それらの間で
ネットワークがメッセージを失っているのとは対照的に、非実用的です。
それを行う1つの方法は、人間に決めさせることです：人間はサーバーを
調査して、どれが生きていてどれが死んでいるかを決定できます。

もう一つのアプローチは、スプリットブレイン操作を許可し、分断が
修復された後に結果として生じる分岐状態をサーバーが調整する方法を
持つことです。これは一部の種類のサービスに対して機能させることが
できますが、複雑なクライアントから見えるセマンティクス（通常
「結果整合性」と呼ばれる）を持ちます。コースで後で割り当てられる
COPS、FuzzyLog、Bitcoin論文を見てください。

----------

Q: Raftでは、レプリケートされているサービスは選挙プロセス中は
クライアントが利用できません。実際にこれはどの程度問題を引き起こしますか？

A: クライアントから見える停止は10分の1秒程度のようです。著者は故障
（したがって選挙）が稀であることを期待しています。マシンやネットワーク
が故障した場合にのみ発生するからです。多くのサーバーとネットワークは
数ヶ月または数年間継続的に稼働し続けるため、これは多くのアプリケーション
にとって大きな問題ではないように思われます。

----------

Q: リーダー選挙の停止がない他のコンセンサスシステムはありますか？

A: リーダーや選挙がないPaxosベースのレプリケーションのバージョンがあり、
したがって選挙中の停止に悩まされません。代わりに、どのサーバーも
いつでも効果的にリーダーとして動作できます。リーダーがないことの
コストは、各合意にはより多くのメッセージが必要なことです。

----------

Q: RaftとVMware FTはどのように関連していますか？

A: Raftには単一障害点がありませんが、VMware FTにはtest-and-setサーバー
の形で単一障害点があります。その意味で、RaftはVMware FTよりも
根本的により耐障害性があります。これは、FTのtest-and-setサーバーを
RaftまたはPaxosを使用してレプリケートされたサービスとして実装すること
で修正できます。

VMware-FTはあらゆる仮想マシンゲストをレプリケートでき、したがって
あらゆるサーバースタイルのソフトウェア、レプリケートされていることを
知らないソフトウェアでさえもレプリケートできます。Raftは、通常レプリ
ケーションでうまく動作するように特別に設計されたアプリケーションソフト
ウェアに統合されたライブラリとして使用されます。

----------

Q: なぜ悪意のある人がRaftサーバーを乗っ取ったり、不正なRaftメッセージ
を偽造したりできないのですか？

A: Raftには、このような攻撃に対する防御が含まれていません。すべての
参加者がプロトコルに従っており、正しいサーバーセットのみが参加している
と仮定しています。

実際のデプロイメントでは、悪意のある攻撃者を排除する必要があります。
最も簡単なオプションは、インターネット上のランダムな人々からのパケット
をフィルタリングするためにファイアウォールの後ろにサーバーを配置し、
ファイアウォール内のすべてのコンピューターと人々が信頼できることを
保証することです。

Raftが潜在的な攻撃者と同じネットワーク上で動作する必要がある状況が
あるかもしれません。その場合、良い計画は何らかの暗号化スキームで
Raftパケットを認証することです。例えば、各正当なRaftサーバーに
公開鍵/秘密鍵ペアを与え、送信するすべてのパケットに署名させ、
各サーバーに正当なRaftサーバーの公開鍵のリストを与え、そのリスト
のキーで署名されていないパケットをサーバーに無視させます。

----------

Q: 論文では、Raftはすべての非ビザンチン条件下で動作すると述べています。
ビザンチン条件とは何で、なぜRaftを失敗させる可能性があるのですか？

A: 「非ビザンチン条件」とは、サーバーが故障停止であることを意味します：
Raftプロトコルを正しく実行するか、停止するかのいずれかです。例えば、
ほとんどの停電は非ビザンチンです。コンピューターが単に命令の実行を
停止するからです; 停電が発生した場合、Raftは動作を停止するかも
しれませんが、クライアントに不正な結果を送信することはありません。

ビザンチン故障とは、バグや誰かが悪意でコンピューターを制御している
ために、一部のコンピューターが不正に実行する状況を指します。このような
故障が発生した場合、Raftはクライアントに不正な結果を送信する可能性
があります。

6.824のほとんどは、非ビザンチン故障を許容することについてです。
ビザンチン故障にもかかわらず正しい動作はより困難です; 学期の終わり
にこのトピックに触れます。

----------

Q: Raftクラスターが同じ物理的な場所にプロビジョニングされると仮定
されていますか、それとも地理的に分散されたデータセンターにピア
をデプロイできますか？

A: 典型的なデプロイメントは単一のデータセンターです。後でデータセンター
間でPaxosを実行するシステム（例：Spanner）を見ますが、これはクライアント
がローカルピア（潜在的に遠いリーダーの代わりに）と話すことができる
ように、リーダーレス設計でより良く行われます。

Googleのチュビー論文は、彼らのチュビーデプロイメントが通常単一の
データセンターであることを報告していますが、ルートチュビー（地理的
に分離されたデータセンターにまたがる）を除きます。（チュビーはRaft
に基づいていませんが、Paxosに基づいたGoogleのレプリケートされた
ステートマシンライブラリを使用しています。）

----------

Q: 操作の厳密な順序付けを必要としないRaftコンセンサスアルゴリズム
のバリエーションはありますか？（つまり、リーダー完全性プロパティ
に従う必要がない。）

A: はい、操作が可換かどうかを知っている場合です。「generalized paxos」
または「exploiting commutativity for practical fast replication」を
Googleで検索してください。

----------

Q: 図1で、クライアントとサーバー間のインターフェースはどのように見えますか？

A: 通常、サーバーへのRPCインターフェースです。Lab 3で構築するような
キー/値ストレージサーバーの場合、Put(key,value)とGet(value) RPCです。
RPCはサーバーのキー/値モジュールによって処理され、RaftにクライアントRPC
をログに記録するようにRaft.Start()を呼び出し、新しくコミットされた
ログエントリを学ぶためにapplyChを読み取ります。

----------

Q: クライアントがリーダーにリクエストを送信したが、リーダーがクライアント
リクエストをすべてのフォロワーに送信する前にクラッシュし、新しい
リーダーがそのリクエストをログに持っていない場合はどうなりますか？
それはクライアントリクエストが失われることになりませんか？

A: はい、リクエストは失われる可能性があります。ログエントリがコミット
されていない場合、Raftはリーダー変更を通してそれを保持しない可能性
があります。

Raftがリクエストをコミットしなかった場合、クライアントはそのリクエスト
への返答を受信できなかったため、それは問題ありません。クライアントは
（タイムアウトまたはリーダー変更を見ることで）そのリクエストが失われた
可能性があることを知り、それを再送信します。

クライアントがリクエストを再送信できるということは、システムが重複
リクエストに対して警戒する必要があることを意味します; Lab 3でこれ
を扱います。

----------

Q: ネットワーク分断がある場合、Raftは2つのリーダーとスプリットブレイン
を持つことになりますか？

A: いいえ。最大1つのアクティブなリーダーしか存在できません。

新しいリーダーは、RequestVote RPCでサーバーの多数（自分自身を含む）
に接触できる場合にのみ選出されます。分断がある場合、分断の1つが
サーバーの多数を含んでいれば、その分断が新しいリーダーを選出できます。
他の分断は少数のみを持つ必要があるため、リーダーを選出できません。
多数の分断がない場合、リーダーはいません（誰かがネットワークを修復
するまで）。

----------

Q: ネットワークが分断されている間に新しいリーダーが選出されたが、
古いリーダーが別の分断にいるとします。古いリーダーは新しいエントリ
のコミットを停止することをどのように知りますか？

A: 古いリーダーは、AppendEntries RPCに対する成功応答の多数を得る
ことができないか（少数派分断にいる場合）、多数と話すことができれば、
その多数は新しいリーダーの多数と重複する必要があり、重複部分の
サーバーが古いリーダーにより高い期間があることを教えます。それは
古いリーダーをフォロワーに切り替えることになります。

----------

Q: 一部のサーバーが故障した場合、「多数」はライブサーバーの多数を
指しますか、それともすべてのサーバー（死んだものも含む）の多数を
指しますか？

A: 常にすべてのサーバーの多数です。したがって、合計5つのRaftピア
がいるが2つが故障した場合、候補者はリーダーに選出されるために
（自分自身を含む）3票を得る必要があります。

これには多くの理由があります。2つの「故障した」サーバーが実際には
別の分断で稼働している可能性があります。彼らの観点から見ると、
3つのサーバーが故障しています。彼らが2票だけ（2つのライブに見える
サーバーから）を使ってリーダーを選出することが許されていれば、
スプリットブレインを得ることになります。もう一つの理由は、新しい
リーダーが前の期間番号と前の期間でコミットされたログエントリを
見ることを保証するために、任意の2つのリーダーの多数が少なくとも
1つのサーバーで重複する必要があることです; これには、死んでいる
ものと生きているものを含むすべてのサーバーからの多数が必要です。

----------

Q: 選挙タイムアウトが短すぎる場合はどうなりますか？それはRaftの
誤動作を引き起こしますか？

A: 選挙タイムアウトの悪い選択は安全性に影響せず、活性にのみ影響します。

選挙タイムアウトが小さすぎる場合、フォロワーはリーダーがAppendEntries
を送信する機会を得る前に繰り返しタイムアウトするかもしれません。
その場合、Raftは新しいリーダーの選出にすべての時間を費やし、
クライアントリクエストの処理に時間を費やさない可能性があります。
選挙タイムアウトが大きすぎる場合、リーダー故障後に新しいリーダー
が選出される前に不必要に長い停止があります。

----------

Q: なぜ選挙タイムアウトをランダム化するのですか？

A: 複数のピアが同時に候補者になり、票を自分たちの間で分割して
誰も多数を得られないという可能性を減らすためです。

----------

Q: 候補者は多数から票を受け取った瞬間に自分をリーダーと宣言し、
さらなるRequestVote返答を待つ必要がないのですか？

A: はい -- 多数で十分です。一部のピアが故障してしたがって応答しない
可能性があるため、より長く待つのは間違いです。

----------

Q: Raftはどのようなネットワークを仮定していますか？

A: ネットワークは信頼できません：リクエストと返答を失い、遅延させる
可能性があります。RaftのRPCライブラリは信頼性を提供しません;
ベストエフォート（例：リクエストを送信するが、ネットワークが
それを落とす可能性がある）です。ラボのRPCライブラリは同様の
セマンティクスを提供します：リクエストを失い、返答を失い、
メッセージを遅延させ、特定のホストを完全に切断する可能性があります。

----------

Q: requestVote RPCでのvotedForチェックの目的は何ですか？

A: 2つの候補者が同じ新しい期間に対して同時に選挙を開始する可能性
があります。フォロワーはそのうち1つにのみ投票する必要があります。

----------

Q: リーダーはクラッシュ以外でリーダーであることを停止できますか？

A: はい。リーダーのCPUが遅い場合、またはネットワーク接続が切断
される場合、または多くのパケットを失う場合、またはパケットの
配信が遅すぎる場合、他のサーバーはそのAppendEntries RPCを見ず、
選挙を開始します。

----------

Q: フォロワーのログエントリはいつステートマシンに送信されますか？

A: AppendEntries RPCのleaderCommitフィールドを使用して、リーダー
がエントリがコミットされたと言った後のみです。その時点で、フォロワー
はログエントリを実行（または適用）でき、これは私たちにとって
applyChで送信することを意味します。

----------

Q: リーダーはAppendEntries RPCへの返答を待つべきですか？

A: リーダーは待機せずに、AppendEntries RPCを並行して送信する必要
があります。返答が戻ってくると、リーダーはそれらをカウントし、
サーバーの多数（自分自身を含む）から返答がある場合にのみ、ログ
エントリをコミット済みとしてマークする必要があります。

Goでこれを行う1つの方法は、リーダーが各AppendEntries RPCを別の
goroutineで送信することで、リーダーがRPCを並行して送信できるよう
にすることです。次のようなものです：

```go
for each server {
  go func() {
    AppendEntries RPCを送信し、返答を待つ
    if reply.success == true {
      カウントをインクリメント
      if count == nservers/2 + 1 {
        このエントリはコミット済み
      }
    }
  } ()
}
```

----------

Q: サーバーの半数（またはそれ以上）が死んだ場合はどうなりますか？

A: サービスは進行できません; リーダーを選出することを何度も何度も
試み続けます。十分なサーバーが永続的なRaft状態を保持したまま
復活した場合、リーダーを選出して継続できます。

----------

Q: なぜRaftログは1インデックスなのですか？

A: 0インデックスとして見る必要がありますが、期間0を持つエントリ
（index=0で）から始まります。これにより、最初のAppendEntries RPC
がPrevLogIndexとして0を含み、ログへの有効なインデックスになることが
できます。

----------

Q: ネットワークが分断された場合、少数派分断のクライアントリクエスト
は失われませんか？

A: サーバーの多数を持つ分断のみがクライアント操作をコミットして
実行できます。少数派分断のサーバーはクライアント操作をコミット
できないため、クライアントリクエストに返答しません。クライアント
は多数Raft分断に接触できるまでリクエストを再送信し続けるため、
これらのクライアントのリクエストは永遠に失われることはありません。

----------

Q: 5.4.3の議論は完全な証明ですか？

A: 5.4.3は完全な証明ではありません。調べる場所はこちらです：

<http://ramcloud.stanford.edu/~ongaro/thesis.pdf>
<http://verdi.uwplse.org/raft-proof.pdf>

----------

Q: Raftの上に構築できるアプリケーションに制限はありますか？

A: レプリケートされたステートマシンフレームワークのようなRaftにきれい
に適合するためには、レプリケートされたサービスは自己完結型である
必要があると思います -- プライベート状態を持ち、状態を更新する
クライアントからのコマンドを受け入れることができますが、特別な
予防措置なしに外部エンティティに接触することはできません。レプリ
ケートされたアプリケーションが外部世界と相互作用する場合、外部
世界は繰り返しリクエスト（レプリケーションとリブート後のログの
再生による）を正しく処理でき、それ自体と矛盾することがない（つまり、
すべてのレプリカと、ログエントリのすべての再実行に対して正確に
同じ答えを送信するよう注意する必要がある）必要があります。それは
順番に、Raftベースのアプリケーションが接触するいかなる外部エンティティ
もそれ自体がフォルトトレラントでなければならないことを要求するよう
に思われます。つまり、おそらくRaftまたはそれに似たものを使用する
必要があります。それはかなり制限的です。

例として、クレジットカード処理リクエストを何らかの外部クレジット
カード処理サービスに送信するレプリケートされたオンライン注文システム
を想像してください。その外部処理装置は繰り返しリクエスト（各レプリカ
から1つまたは複数）を見るでしょう。各リクエストに対して正確に同じ
方法で応答するでしょうか？クレジットカードを複数回請求するでしょうか？
正しいことを行う場合でも、不適切なタイミングでクラッシュしてリブート
する場合でも正しいことを行うでしょうか？

----------

Q: セクション7のコピーオンライト最適化とは何ですか？

A: 基本的なアイデアは、サービスがスナップショットを作成したい場合に
サーバーがfork()を行い、子にメモリ内状態の完全なコピーを与えることです。
fork()が実際にすべてのメモリをコピーし、状態が大きい場合、これは
遅くなります。しかし、ほとんどのオペレーティングシステムはfork()で
すべてのメモリをコピーしません; 代わりに、メモリページを「コピー
オンライト」としてマークし、親と子の両方で読み取り専用にします。
その後、サーバーがページを書き込もうとすると、オペレーティング
システムはページフォルトを見て、その時点でのみページをコピーします。
正味の効果は、通常、子がfork()時の親プロセスのメモリのコピーを見る
ことですが、比較的少ないコピーで済むことです。（この最適化はあなた
のラボには必要ありません。）

----------

Q: なぜRaftと呼ばれるのですか？

A: <https://groups.google.com/g/raft-dev/c/95rZqptGpmU>

----------

Q: Raft/Paxosが正しく動作することは重要ですか？

A: Raft/Paxosは分散システムの基盤であることが多いです。例えば、
Googleの多くのサービス（GFS、Spanner、BigTable等）は、Paxosに
基づく構成サービスであるChubbyに依存しています。別の例として、
多くの企業がコンテナを管理するためにKubernetesを使用しています；
Kubernetesは順番に、Raftに基づくキー/値サービスであるEtcdに
コンテナに関する構成情報を保存します。時折、Paxos/Raftに基づく
システムの問題が実現し、重大な停止につながります; 例として、
<https://decentralizedthoughts.github.io/2020-12-12-raft-liveness-full-omission/>
を参照してください。
